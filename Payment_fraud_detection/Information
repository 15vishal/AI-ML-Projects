1. Introduction to the Project
The goal of this project is to detect fraudulent transactions in an online payment system using machine learning techniques. The dataset contains information about transactions, including features like transaction amount, transaction type, and account balances. The target variable is whether a transaction is fraudulent or not.

2. Data Loading and Exploration
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Loading the dataset in pandas dataframe
data = pd.read_csv('/content/sample_data/PS_20174392719_1491204439457_log.csv')
data.head()

data.info()

data.describe()

data.isnull().sum()
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Explanation:
Loading the Data: The dataset is loaded into a pandas DataFrame. You can use data.head() to view the first few rows, data.info() to get a summary of the DataFrame, and data.describe() to get statistical details.
Missing Values: data.isnull().sum() shows the number of missing values in each column.

3. Data Cleaning
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# Drop rows with missing values
data_cleaned = data.dropna()

data_cleaned.shape
data_cleaned['isFraud'].value_counts()
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Explanation:
Handling Missing Values: Rows with missing values are removed using dropna(). This is a straightforward method, but depending on the extent of missing data, you might need to consider imputation.
Dataset Shape: After cleaning, data_cleaned.shape shows the new size of the dataset.
Target Distribution: data_cleaned['isFraud'].value_counts() shows the distribution of the target variable (isFraud), which helps in understanding the class balance (i.e., how many frauds vs. non-frauds).

4. Feature Selection and Target Variable
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# Load the features to a variable X
X = data_cleaned[['step', 'type', 'amount', 'nameOrig', 'oldbalanceOrg', 'newbalanceOrig', 'nameDest', 'oldbalanceDest', 'newbalanceDest']]

# Load the target variable to y
y = data_cleaned['isFraud']
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Explanation:
Feature Selection: Selects relevant columns for features (X) and the target variable (y). Ensure features are appropriately chosen based on domain knowledge and relevance.

5. Data Splitting
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
from sklearn.model_selection import train_test_split

# Split data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Explanation:
Train-Test Split: The dataset is split into training and testing sets using an 80-20 split. The random_state ensures reproducibility of the split.

6. Preprocessing and Model Pipeline
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

# Define preprocessing for categorical and numerical features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']),
        ('cat', OneHotEncoder(handle_unknown='ignore'), ['step', 'type', 'nameOrig', 'nameDest'])
    ])

# Create pipeline
pipeline = Pipeline([
    ('preprocessor', preprocessor),  # Preprocessing steps
    ('classifier', LogisticRegression(solver='liblinear', random_state=42))  # Model
])
pipeline.fit(X_train, y_train)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Explanation:
Preprocessing: ColumnTransformer applies different preprocessing steps to numerical and categorical features.
Numerical Features: Scaled using StandardScaler.
Categorical Features: Encoded using OneHotEncoder to handle categorical variables.
Pipeline: Combines preprocessing and model training into a single workflow. LogisticRegression is used as the classification model.

7. Making Predictions and Evaluating the Model
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# Make predictions
y_pred = pipeline.predict(X_test)

# Evaluate the model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

precision = precision_score(y_test, y_pred, average='binary')  # 'binary' for binary classification
print(f"Precision: {precision}")

recall = recall_score(y_test, y_pred, average='binary')
print(f"Recall: {recall}")

f1 = f1_score(y_test, y_pred, average='binary')
print(f"F1 Score: {f1}")

# Generate the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

import seaborn as sn
plt.figure(figsize=(10,7))
sn.heatmap(cm, annot=True)
plt.title('Confusion Matrix - Test Data')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Explanation:
Predictions: Predicts the target variable for the test data.

Evaluation Metrics:
Accuracy: Proportion of correct predictions.
Precision: Proportion of true positive predictions out of all positive predictions.
Recall: Proportion of true positive predictions out of all actual positives.
F1 Score: Harmonic mean of precision and recall.
Confusion Matrix: A matrix showing the counts of true positive, true negative, false positive, and false negative predictions. Visualized using a heatmap with seaborn.

Conclusion:
Data Cleaning: Removing rows with missing values.
Feature Engineering: Selecting and preprocessing features.
Model Building: Using Logistic Regression with preprocessing steps in a pipeline.
Evaluation: Assessing model performance with accuracy, precision, recall, F1-score, and confusion matrix.
